{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYavbgS8BlsQvKvrD/s1e2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeanFord7/CMM307-AdvancedArtificialIntelligence/blob/main/DeanFord1702994-CMM307Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 - Dataset"
      ],
      "metadata": {
        "id": "0E4Ts2HCAxkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Twitter Sentiment Analysis (TSA) dataset contains over 70,000 records of tweets related to specific entities, whether that be a compnay, a game etc. Each record has four columns, an ID for the tweet, the entity the tweet is referencing, the text conatained within the tweet and the sentiment. <br><br>\n",
        "The aim of the task is to use the text of each tweet to predict and assign a sentiment classification of one of the following to the tweet:\n",
        "<ul>\n",
        "<li>Positive</li>\n",
        "<li>Negative</li>\n",
        "<li>Neutral</li>\n",
        "<li>Irrelevant</li>\n",
        "</ul>\n",
        "In the dataset, 'Neutral' and 'Irrelevant' are seperate labels for the sentiment classification but both are considered to be the same result."
      ],
      "metadata": {
        "id": "MWtAu0LdbBoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "zbP3KXklTKp9",
        "outputId": "93c2853c-362b-402d-8b3b-426f15cd5ed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jp797498e/twitter-entity-sentiment-analysis/versions/2\n",
            "Files in directory: ['twitter_training.csv', 'twitter_validation.csv']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Load dataset from kagglehub\n",
        "path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "files = os.listdir(path)\n",
        "print(\"Files in directory:\", files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dataset is already split into a training and testing file so retrieve both\n",
        "training_file_path = os.path.join(path, \"twitter_training.csv\")\n",
        "validation_file_path = os.path.join(path, \"twitter_validation.csv\")\n",
        "\n",
        "column_names = [\"tweet_id\", \"entity\", \"sentiment\", \"tweet_text\"]\n",
        "\n",
        "# Load datasets into dataframes\n",
        "training_df = pd.read_csv(training_file_path, names=column_names)\n",
        "validation_df = pd.read_csv(validation_file_path, names=column_names)\n",
        "\n",
        "#print(training_df.head())\n",
        "print(\"Train\", len(training_df))\n",
        "print(\"Val\", len(validation_df))\n",
        "#print(validation_df.head())\n",
        "\n",
        "# Combine the dataframes as the pre made validation set has only 1,000 values compared to the testing sets 69,000\n",
        "sentiment_df = pd.concat([training_df, validation_df], ignore_index=True)\n",
        "\n",
        "print(\"Combined Dataframe:\")\n",
        "print(sentiment_df.head())\n",
        "print(\"Records: \", len(sentiment_df))\n",
        "\n",
        "# The dataset contains 6 records for each tweet with the first being the original and the next 5 being slight alterations of the text\n",
        "# Remove the additional records for each tweet as the original text in the most relevant and the duplicates only have minor grammatical changes\n",
        "filtered_sentiment_df = sentiment_df.drop_duplicates(subset=[\"tweet_id\"], keep=\"first\")\n",
        "\n",
        "filtered_sentiment_df = filtered_sentiment_df[filtered_sentiment_df['tweet_text'].notnull()]  # Remove NaN values\n",
        "filtered_sentiment_df = filtered_sentiment_df[filtered_sentiment_df['tweet_text'].str.strip() != '']  # Remove empty strings\n",
        "\n",
        "# Change sentiment values from 'Irrelevant' to 'Neutral'\n",
        "# 'Irrelevant' and 'Neutral' are treated as the same result in the dataset so convert all to 'Neutral' to avoid confusion in the results\n",
        "filtered_sentiment_df.loc[filtered_sentiment_df['sentiment'] == 'Irrelevant', 'sentiment'] = 'Neutral'\n",
        "\n",
        "print(\"Filtered Dataframe:\")\n",
        "print(filtered_sentiment_df.head())\n",
        "print(\"Records: \", len(filtered_sentiment_df))\n"
      ],
      "metadata": {
        "id": "VunrCDJMJQ94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368a7db0-559f-4a9e-a7eb-1af56129812e"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 74682\n",
            "Val 1000\n",
            "Combined Dataframe:\n",
            "   tweet_id       entity sentiment  \\\n",
            "0      2401  Borderlands  Positive   \n",
            "1      2401  Borderlands  Positive   \n",
            "2      2401  Borderlands  Positive   \n",
            "3      2401  Borderlands  Positive   \n",
            "4      2401  Borderlands  Positive   \n",
            "\n",
            "                                          tweet_text  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "Records:  75682\n",
            "Filtered Dataframe:\n",
            "    tweet_id       entity sentiment  \\\n",
            "0       2401  Borderlands  Positive   \n",
            "6       2402  Borderlands  Positive   \n",
            "12      2403  Borderlands   Neutral   \n",
            "18      2404  Borderlands  Positive   \n",
            "24      2405  Borderlands  Negative   \n",
            "\n",
            "                                           tweet_text  \n",
            "0   im getting on borderlands and i will murder yo...  \n",
            "6   So I spent a few hours making something for fu...  \n",
            "12  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n",
            "18  that was the first borderlands session in a lo...  \n",
            "24  the biggest dissappoinment in my life came out...  \n",
            "Records:  12275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = list(filtered_sentiment_df['tweet_text'])\n",
        "sentiments = list(filtered_sentiment_df['sentiment'])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "sentiments_numerical = label_encoder.fit_transform(sentiments)\n",
        "\n"
      ],
      "metadata": {
        "id": "OpY3srQ7OwPB"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def prep(sentences):\n",
        "  prep_text = []\n",
        "\n",
        "  words = stopwords.words('english')\n",
        "  entity_labels = list(filtered_sentiment_df['entity'])\n",
        "  words.extend(entity_labels)\n",
        "\n",
        "  for sent in sentences:\n",
        "    token_text = word_tokenize(sent)\n",
        "    normalised_text = [token.lower() for token in token_text if token.isalpha()]\n",
        "\n",
        "    swr_text = [token for token in normalised_text if token not in words]\n",
        "\n",
        "    prep_text += [[lemmatizer.lemmatize(word) for word in swr_text]]\n",
        "  prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "  return prep_sentences\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzIpSi0-BM1l",
        "outputId": "2f5b3b25-fd71-49fc-95bf-944746465975"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 - Representation Learning"
      ],
      "metadata": {
        "id": "tXuMzQMnBJsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_base = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))"
      ],
      "metadata": {
        "id": "uzKGJIhi3Zpu"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 -  Algorithms"
      ],
      "metadata": {
        "id": "4PVKKM8DBMLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
      ],
      "metadata": {
        "id": "ZnRkt3z7xkfT"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "ZmHCiQdWUa0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(dataset_size, num_classes):\n",
        "  input_shape = (dataset_size,)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax')) #remember this format from last year? We effectively parameterse two hidden layers and one output layer\n",
        "  return model\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "xnp = np.array(tweets)\n",
        "ynp = np.array(sentiments_numerical)\n",
        "\n",
        "mlp_accuracy_score = []\n",
        "\n",
        "for train, test in kf.split(xnp,ynp):\n",
        "  x_train, x_test, y_train, y_test = xnp[train], xnp[test], ynp[train], ynp[test]\n",
        "\n",
        "  x_train = prep(x_train)\n",
        "  x_test = prep(x_test)\n",
        "\n",
        "  tfidf = tfidf_base\n",
        "  x_train = tfidf.fit_transform(x_train)\n",
        "  x_train = x_train.todense()\n",
        "  x_test = tfidf.transform(x_test)\n",
        "  x_test = x_test.todense()\n",
        "  num_classes = len(np.unique(y_train))\n",
        "  model = mlp(1000, num_classes)\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=250, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "  test_results = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
        "\n",
        "  mlp_accuracy_score.append(test_results[1])\n",
        "\n",
        "\n",
        "print(\"MLP Accuracy:\", np.mean(mlp_accuracy_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpG2jUnek29X",
        "outputId": "f7a7cf5b-a6a7-4657-9953-56a22dbe70e7"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4126 - loss: 1.0827 - val_accuracy: 0.4012 - val_loss: 1.0538\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5003 - loss: 0.9641 - val_accuracy: 0.5474 - val_loss: 0.9097\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6608 - loss: 0.7636 - val_accuracy: 0.5616 - val_loss: 0.9332\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7088 - loss: 0.6908 - val_accuracy: 0.5611 - val_loss: 0.9572\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7345 - loss: 0.6263 - val_accuracy: 0.5611 - val_loss: 1.0187\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7551 - loss: 0.5901 - val_accuracy: 0.5550 - val_loss: 1.0560\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7886 - loss: 0.5334 - val_accuracy: 0.5489 - val_loss: 1.1509\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5880 - loss: 0.8809\n",
            "Test results - Loss: 0.8807139992713928 - Accuracy: 0.5885946750640869%\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.4185 - loss: 1.0828 - val_accuracy: 0.4078 - val_loss: 1.0537\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.4782 - loss: 0.9813 - val_accuracy: 0.5774 - val_loss: 0.9023\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6810 - loss: 0.7670 - val_accuracy: 0.5662 - val_loss: 0.8936\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7100 - loss: 0.6850 - val_accuracy: 0.5636 - val_loss: 0.9423\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7352 - loss: 0.6348 - val_accuracy: 0.5550 - val_loss: 0.9908\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7684 - loss: 0.5774 - val_accuracy: 0.5519 - val_loss: 1.0328\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7893 - loss: 0.5348 - val_accuracy: 0.5418 - val_loss: 1.1432\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8112 - loss: 0.4861 - val_accuracy: 0.5412 - val_loss: 1.2126\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5865 - loss: 0.8943\n",
            "Test results - Loss: 0.8847615718841553 - Accuracy: 0.591446042060852%\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.3924 - loss: 1.0844 - val_accuracy: 0.3931 - val_loss: 1.0617\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4710 - loss: 0.9809 - val_accuracy: 0.5545 - val_loss: 0.9201\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6673 - loss: 0.7794 - val_accuracy: 0.5743 - val_loss: 0.9149\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7079 - loss: 0.6831 - val_accuracy: 0.5667 - val_loss: 0.9703\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7237 - loss: 0.6452 - val_accuracy: 0.5631 - val_loss: 1.0245\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7589 - loss: 0.5889 - val_accuracy: 0.5555 - val_loss: 1.1121\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8000 - loss: 0.5184 - val_accuracy: 0.5453 - val_loss: 1.1820\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8298 - loss: 0.4733 - val_accuracy: 0.5494 - val_loss: 1.2513\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5962 - loss: 0.8545\n",
            "Test results - Loss: 0.8649817109107971 - Accuracy: 0.590224027633667%\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.4012 - loss: 1.0827 - val_accuracy: 0.3859 - val_loss: 1.0591\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.4994 - loss: 0.9674 - val_accuracy: 0.5703 - val_loss: 0.9153\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6587 - loss: 0.7728 - val_accuracy: 0.5759 - val_loss: 0.9100\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7030 - loss: 0.6908 - val_accuracy: 0.5636 - val_loss: 0.9494\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7416 - loss: 0.6254 - val_accuracy: 0.5336 - val_loss: 1.0046\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7598 - loss: 0.5857 - val_accuracy: 0.5168 - val_loss: 1.0757\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7867 - loss: 0.5314 - val_accuracy: 0.5107 - val_loss: 1.1496\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8300 - loss: 0.4615 - val_accuracy: 0.5005 - val_loss: 1.2520\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6104 - loss: 0.8483\n",
            "Test results - Loss: 0.8652387261390686 - Accuracy: 0.6020366549491882%\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.4162 - loss: 1.0812 - val_accuracy: 0.4063 - val_loss: 1.0480\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5002 - loss: 0.9748 - val_accuracy: 0.5779 - val_loss: 0.8981\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6681 - loss: 0.7739 - val_accuracy: 0.5835 - val_loss: 0.9020\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6960 - loss: 0.7113 - val_accuracy: 0.5835 - val_loss: 0.9154\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7162 - loss: 0.6507 - val_accuracy: 0.5692 - val_loss: 0.9946\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7726 - loss: 0.5717 - val_accuracy: 0.5626 - val_loss: 1.0376\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7911 - loss: 0.5357 - val_accuracy: 0.5606 - val_loss: 1.0975\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6053 - loss: 0.8655\n",
            "Test results - Loss: 0.869724690914154 - Accuracy: 0.6069246530532837%\n",
            "MLP Accuracy: 0.5958452105522156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-Nearest Neighbour"
      ],
      "metadata": {
        "id": "er51nR02sdL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "knn_accuracy_score = []\n",
        "\n",
        "for train, test in kf.split(xnp,ynp):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = xnp[train], xnp[test], ynp[train], ynp[test]\n",
        "\n",
        "  x_train = prep(x_train)\n",
        "  x_test = prep(x_test)\n",
        "\n",
        "  tfidf = tfidf_base\n",
        "  x_train = tfidf.fit_transform(x_train)\n",
        "  x_train = np.asarray(x_train.todense())\n",
        "  x_test = tfidf.transform(x_test)\n",
        "  x_test = np.asarray(x_test.todense())\n",
        "  param_grid = {\n",
        "        'n_neighbors': [1, 3, 5, 7]\n",
        "    }\n",
        "\n",
        "  grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy')\n",
        "  grid_search.fit(x_train, y_train)\n",
        "\n",
        "  # Get the best model\n",
        "  best_kNN = grid_search.best_estimator_\n",
        "\n",
        "  knn_predictions = best_kNN.predict(x_test)\n",
        "  knn_acc = accuracy_score(knn_predictions, y_test)\n",
        "  knn_accuracy_score.append(knn_acc)\n",
        "\n",
        "print(\"kNN Accuracy:\", np.mean(knn_accuracy_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4I-P5ORsld7",
        "outputId": "b91cf600-7ac0-4668-dbb8-3993f1cab3b6"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN Accuracy: 0.48570264765784116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "pJKPBNih7xkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code taken and adapted from https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n",
        "\n",
        "from sklearn import naive_bayes\n",
        "\n",
        "nb_accuracy_scores = []\n",
        "\n",
        "# Encode labels\n",
        "Encoder = LabelEncoder()\n",
        "ynp_encoded = Encoder.fit_transform(ynp)  # Encoding target labels\n",
        "\n",
        "for train_idx, test_idx in kf.split(xnp, ynp_encoded):\n",
        "    # Split data into train and test sets for this fold\n",
        "    x_train, x_test = np.array(tweets)[train_idx], np.array(tweets)[test_idx]\n",
        "    y_train, y_test = ynp_encoded[train_idx], ynp_encoded[test_idx]\n",
        "\n",
        "    x_train = prep(x_train)\n",
        "    x_test = prep(x_test)\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf = tfidf_base\n",
        "    x_train = tfidf.fit_transform(x_train)\n",
        "    x_train = np.asarray(x_train.todense())\n",
        "    x_test = tfidf.transform(x_test)\n",
        "    x_test = np.asarray(x_test.todense())\n",
        "\n",
        "    naive = naive_bayes.MultinomialNB()\n",
        "    naive.fit(x_train, y_train)\n",
        "\n",
        "    nb_predictions = naive.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, nb_predictions)\n",
        "    nb_accuracy_scores.append((accuracy) * 100)\n",
        "\n",
        "print(f\"Naive Bayes Accuracy:\", np.mean(nb_accuracy_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dv1qzHp72_O",
        "outputId": "70cda3ac-2151-499e-9763-42fdc827fdc6"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 59.576374745417525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4 - Evaluation"
      ],
      "metadata": {
        "id": "DfqtLga5BR47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5 - Paper Overview"
      ],
      "metadata": {
        "id": "9-WoNFmsBUsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6 - Algorithms"
      ],
      "metadata": {
        "id": "vYCgfIFWBXVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "CQgnhlC6LSCc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "random_embedding_dimension = 32\n",
        "glove_embedding_dimension = 200\n",
        "max_length = 40\n",
        "lstm_units_random = 100\n",
        "lstm_units_glove = 128\n",
        "dense_units_random = 32\n",
        "dense_units_glove = 64"
      ],
      "metadata": {
        "id": "uYgK5TC2BduQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The paper only considers positive and negative sentiments for the algorithm\n",
        "# Because of this we filter the dataset to only include these rows for use in the models\n",
        "binary_sentiment_df = filtered_sentiment_df.drop(filtered_sentiment_df[filtered_sentiment_df['sentiment'] == 'Neutral'].index)\n",
        "\n",
        "binary_tweets = list(binary_sentiment_df['tweet_text'])\n",
        "binary_tweets = prep(binary_tweets)\n",
        "binary_sentiments = list(binary_sentiment_df['sentiment'])\n",
        "\n",
        "# Convert the sentiments to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "binary_sentiments_numerical = label_encoder.fit_transform(binary_sentiments)"
      ],
      "metadata": {
        "id": "d0LJUaBQpGX9"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(binary_tweets)\n",
        "\n",
        "# Convert the tweets into sequneces and pad them to fit the max length of 40\n",
        "sequences = tokenizer.texts_to_sequences(binary_tweets)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "binary_xnp = np.array(padded_sequences)\n",
        "binary_ynp = np.array(binary_sentiments_numerical)"
      ],
      "metadata": {
        "id": "ZFmBmnqwkm8u"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code taken and adapted from:\n",
        "# https://stackoverflow.com/questions/35089956/how-to-use-the-embedding-layer-for-recurrent-neural-network-rnn-in-keras\n",
        "# https://keras.io/api/layers/core_layers/embedding/\n",
        "# https://keras.io/api/layers/regularization_layers/dropout/\n",
        "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "\n",
        "# Add the layers to the random embedding model\n",
        "random_embedding_model = Sequential()\n",
        "random_embedding_model.add(Embedding(input_dim=vocab_size, output_dim=random_embedding_dimension, input_length=max_length))\n",
        "random_embedding_model.add(Dropout(0.2))\n",
        "random_embedding_model.add(LSTM(lstm_units_random))\n",
        "random_embedding_model.add(Dense(dense_units_random, activation='relu'))\n",
        "random_embedding_model.add(Dropout(0.2))\n",
        "random_embedding_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ybbToAKLH6D",
        "outputId": "5d92abdd-24a1-413a-952f-43e7fc3ae4b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(binary_xnp, binary_ynp, test_size=0.2, random_state=42)\n",
        "\n",
        "random_embedding_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = random_embedding_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = random_embedding_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wY2St-KML7f",
        "outputId": "59e46c52-9241-4144-87c0-79880909ab68"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.5061 - loss: 0.6934 - val_accuracy: 0.5411 - val_loss: 0.6929\n",
            "Epoch 2/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - accuracy: 0.5037 - loss: 0.6931 - val_accuracy: 0.5411 - val_loss: 0.6910\n",
            "Epoch 3/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.5401 - loss: 0.6827 - val_accuracy: 0.5411 - val_loss: 0.6796\n",
            "Epoch 4/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step - accuracy: 0.5374 - loss: 0.6560 - val_accuracy: 0.5157 - val_loss: 0.6665\n",
            "Epoch 5/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6036 - loss: 0.6244 - val_accuracy: 0.5726 - val_loss: 0.6490\n",
            "Epoch 6/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.6524 - loss: 0.5929 - val_accuracy: 0.6049 - val_loss: 0.6551\n",
            "Epoch 7/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.6972 - loss: 0.5607 - val_accuracy: 0.5997 - val_loss: 0.6721\n",
            "Epoch 8/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7331 - loss: 0.5277 - val_accuracy: 0.6215 - val_loss: 0.6600\n",
            "Epoch 9/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.7742 - loss: 0.4881 - val_accuracy: 0.6416 - val_loss: 0.6883\n",
            "Epoch 10/10\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.8043 - loss: 0.4550 - val_accuracy: 0.6626 - val_loss: 0.6961\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7048 - loss: 0.6476\n",
            "Test Loss: 0.6359091401100159\n",
            "Test Accuracy: 0.7069929838180542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe embeddings from stanford\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d /content/glove_data\n",
        "\n",
        "# Set the path to the GloVe file to be used\n",
        "glove_file_path = '/content/glove_data/glove.6B.200d.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNvzJx79WQr7",
        "outputId": "57dece53-9008-4d72-a276-ae2a858d4a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-28 20:15:01--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-11-28 20:15:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-11-28 20:15:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.07MB/s    in 3m 12s  \n",
            "\n",
            "2024-11-28 20:18:13 (4.29 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace /content/glove_data/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code taken and adapted from https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
        "\n",
        "glove_embeddings_index = {}\n",
        "\n",
        "# Create the GloVe matrix from the txt file\n",
        "with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            glove_embeddings_index[word] = vector\n",
        "\n",
        "glove_matrix = np.zeros((vocab_size, glove_embedding_dimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index < vocab_size:\n",
        "        embedding_vector = glove_embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            glove_matrix[index] = embedding_vector\n",
        "\n",
        "# Add the layers to the GloVe embedding model\n",
        "glove_embedding_model = Sequential(Embedding(input_dim=vocab_size, output_dim=glove_embedding_dimension, input_length=max_length, weights=[glove_matrix], trainable=True))\n",
        "glove_embedding_model.add(Dropout(0.4))\n",
        "glove_embedding_model.add(LSTM(lstm_units_glove))\n",
        "glove_embedding_model.add(Dense(dense_units_glove, activation='relu'))\n",
        "glove_embedding_model.add(Dropout(0.5))\n",
        "glove_embedding_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "nDAGoDXQL1Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(binary_xnp, binary_ynp, test_size=0.2, random_state=42)\n",
        "\n",
        "glove_embedding_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = glove_embedding_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = glove_embedding_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "uZn2_dXtTi-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7 - Evaluation"
      ],
      "metadata": {
        "id": "wMW5xGIiBa7-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Ws00iF1BdF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}