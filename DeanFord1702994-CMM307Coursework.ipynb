{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOm1oCHvNxu8rTZynbhZvO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeanFord7/CMM307-AdvancedArtificialIntelligence/blob/main/DeanFord1702994-CMM307Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 - Dataset"
      ],
      "metadata": {
        "id": "0E4Ts2HCAxkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Twitter Sentiment Analysis (TSA) dataset contains over 70,000 records of tweets related to specific entities, whether that be a compnay, a game etc. Each record has four columns, an ID for the tweet, the entity the tweet is referencing, the text conatained within the tweet and the sentiment. <br><br>\n",
        "The aim of the task is to use the text of each tweet to predict and assign a sentiment classification of one of the following to the tweet:\n",
        "<ul>\n",
        "<li>Positive</li>\n",
        "<li>Negative</li>\n",
        "<li>Neutral</li>\n",
        "<li>Irrelevant</li>\n",
        "</ul>\n",
        "In the dataset, 'Neutral' and 'Irrelevant' are seperate labels for the sentiment classification but both are considered to be the same result."
      ],
      "metadata": {
        "id": "MWtAu0LdbBoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zbP3KXklTKp9",
        "outputId": "222ad352-e48e-4f38-f0bf-f774b9f777bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jp797498e/twitter-entity-sentiment-analysis/versions/2\n",
            "Files in directory: ['twitter_training.csv', 'twitter_validation.csv']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Load dataset from kagglehub\n",
        "path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "files = os.listdir(path)\n",
        "print(\"Files in directory:\", files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dataset is already split into a training and testing file so retrieve both\n",
        "training_file_path = os.path.join(path, \"twitter_training.csv\")\n",
        "validation_file_path = os.path.join(path, \"twitter_validation.csv\")\n",
        "\n",
        "column_names = [\"tweet_id\", \"entity\", \"sentiment\", \"tweet_text\"]\n",
        "\n",
        "# Load datasets into dataframes\n",
        "training_df = pd.read_csv(training_file_path, names=column_names)\n",
        "validation_df = pd.read_csv(validation_file_path, names=column_names)\n",
        "\n",
        "#print(training_df.head())\n",
        "print(\"Train\", len(training_df))\n",
        "print(\"Val\", len(validation_df))\n",
        "#print(validation_df.head())\n",
        "\n",
        "# Combine the dataframes as the pre made validation set has only 1,000 values compared to the testing sets 69,000\n",
        "sentiment_df = pd.concat([training_df, validation_df], ignore_index=True)\n",
        "\n",
        "print(\"Combined Dataframe:\")\n",
        "print(sentiment_df.head())\n",
        "print(\"Records: \", len(sentiment_df))\n",
        "\n",
        "# The dataset contains 6 records for each tweet with the first being the original and the next 5 being slight alterations of the text\n",
        "# Remove the additional records for each tweet as the original text in the most relevant and the duplicates only have minor grammatical changes\n",
        "filtered_sentiment_df = sentiment_df.drop_duplicates(subset=[\"tweet_id\"], keep=\"first\")\n",
        "\n",
        "filtered_sentiment_df = filtered_sentiment_df[filtered_sentiment_df['tweet_text'].notnull()]  # Remove NaN values\n",
        "filtered_sentiment_df = filtered_sentiment_df[filtered_sentiment_df['tweet_text'].str.strip() != '']  # Remove empty strings\n",
        "\n",
        "# Change sentiment values from 'Irrelevant' to 'Neutral'\n",
        "# 'Irrelevant' and 'Neutral' are treated as the same result in the dataset so convert all to 'Neutral' to avoid confusion in the results\n",
        "filtered_sentiment_df.loc[filtered_sentiment_df['sentiment'] == 'Irrelevant', 'sentiment'] = 'Neutral'\n",
        "\n",
        "print(\"Filtered Dataframe:\")\n",
        "print(filtered_sentiment_df.head())\n",
        "print(\"Records: \", len(filtered_sentiment_df))\n"
      ],
      "metadata": {
        "id": "VunrCDJMJQ94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efbb83d-2e2e-4887-8385-7ec6f8074fdd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 74682\n",
            "Val 1000\n",
            "Combined Dataframe:\n",
            "   tweet_id       entity sentiment  \\\n",
            "0      2401  Borderlands  Positive   \n",
            "1      2401  Borderlands  Positive   \n",
            "2      2401  Borderlands  Positive   \n",
            "3      2401  Borderlands  Positive   \n",
            "4      2401  Borderlands  Positive   \n",
            "\n",
            "                                          tweet_text  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "Records:  75682\n",
            "Filtered Dataframe:\n",
            "    tweet_id       entity sentiment  \\\n",
            "0       2401  Borderlands  Positive   \n",
            "6       2402  Borderlands  Positive   \n",
            "12      2403  Borderlands   Neutral   \n",
            "18      2404  Borderlands  Positive   \n",
            "24      2405  Borderlands  Negative   \n",
            "\n",
            "                                           tweet_text  \n",
            "0   im getting on borderlands and i will murder yo...  \n",
            "6   So I spent a few hours making something for fu...  \n",
            "12  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n",
            "18  that was the first borderlands session in a lo...  \n",
            "24  the biggest dissappoinment in my life came out...  \n",
            "Records:  12275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = list(filtered_sentiment_df['tweet_text'])\n",
        "sentiments = list(filtered_sentiment_df['sentiment'])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "sentiments_numerical = label_encoder.fit_transform(sentiments)\n",
        "\n"
      ],
      "metadata": {
        "id": "OpY3srQ7OwPB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def prep(sentences):\n",
        "  prep_text = []\n",
        "\n",
        "  words = stopwords.words('english')\n",
        "  entity_labels = list(filtered_sentiment_df['entity'])\n",
        "  words.extend(entity_labels)\n",
        "\n",
        "  for sent in sentences:\n",
        "    token_text = word_tokenize(sent)\n",
        "    normalised_text = [token.lower() for token in token_text if token.isalpha()]\n",
        "\n",
        "    swr_text = [token for token in normalised_text if token not in words]\n",
        "\n",
        "    prep_text += [[lemmatizer.lemmatize(word) for word in swr_text]]\n",
        "  prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "  return prep_sentences\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzIpSi0-BM1l",
        "outputId": "4169019a-67bb-4b9c-8156-ec139210784f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 - Representation Learning"
      ],
      "metadata": {
        "id": "tXuMzQMnBJsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "acc_score = []\n",
        "tfidf_base = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))"
      ],
      "metadata": {
        "id": "uzKGJIhi3Zpu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 -  Algorithms"
      ],
      "metadata": {
        "id": "4PVKKM8DBMLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "ZmHCiQdWUa0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def mlp(dataset_size, num_classes):\n",
        "  input_shape = (dataset_size,)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax')) #remember this format from last year? We effectively parameterse two hidden layers and one output layer\n",
        "  return model\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "xnp = np.array(tweets) #convert to numpy to standardise our arrays for the split\n",
        "ynp = np.array(sentiments_numerical)\n",
        "\n",
        "for train, test in kf.split(xnp,ynp):\n",
        "  x_train, x_test, y_train, y_test = xnp[train], xnp[test], ynp[train], ynp[test]\n",
        "\n",
        "  x_train = prep(x_train) #we preprocess our train and test datasets\n",
        "  x_test = prep(x_test)\n",
        "\n",
        "  tfidf = tfidf_base #notice we copy a blank tfidf so there is no leakage\n",
        "  x_train = tfidf.fit_transform(x_train)\n",
        "  x_train = x_train.todense() #by default, tfidf will output a sparse matris to conserve memory. This is incompatible with our deep learner\n",
        "  x_test = tfidf.transform(x_test)\n",
        "  x_test = x_test.todense()\n",
        "  num_classes = len(np.unique(y_train))\n",
        "  model = mlp(1000, num_classes) #we also instantiate a new mlp to prevent leakage of train and test set\n",
        "  y_train = to_categorical(y_train, num_classes) #convert y to one hot vectors\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "  # Configure the model and start training\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #we have parameterised with fairly standard metrics - do feel free to alter and investigate\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=250, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "  # Test the model after training\n",
        "  test_results = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
        "\n",
        "  acc_score.append(test_results[1])\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LpG2jUnek29X",
        "outputId": "3b513ce1-9e51-4abf-be5f-95147e3d94c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.4197 - loss: 1.0806 - val_accuracy: 0.3951 - val_loss: 1.0530\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5118 - loss: 0.9489 - val_accuracy: 0.5708 - val_loss: 0.9244\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7010 - loss: 0.7052 - val_accuracy: 0.5759 - val_loss: 0.9340\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7637 - loss: 0.5884 - val_accuracy: 0.5667 - val_loss: 1.0689\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8115 - loss: 0.5031 - val_accuracy: 0.5484 - val_loss: 1.1553\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8402 - loss: 0.4374 - val_accuracy: 0.5428 - val_loss: 1.2427\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8842 - loss: 0.3676 - val_accuracy: 0.5387 - val_loss: 1.3971\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6029 - loss: 0.8539\n",
            "Test results - Loss: 0.8546436429023743 - Accuracy: 0.6048879623413086%\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.3962 - loss: 1.0821 - val_accuracy: 0.3915 - val_loss: 1.0571\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4987 - loss: 0.9535 - val_accuracy: 0.5743 - val_loss: 0.8975\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7102 - loss: 0.6932 - val_accuracy: 0.5764 - val_loss: 0.9387\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7620 - loss: 0.5777 - val_accuracy: 0.5677 - val_loss: 1.0224\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8189 - loss: 0.4836 - val_accuracy: 0.5657 - val_loss: 1.1110\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8498 - loss: 0.4172 - val_accuracy: 0.5621 - val_loss: 1.2342\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8886 - loss: 0.3447 - val_accuracy: 0.5560 - val_loss: 1.3796\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5792 - loss: 0.8742\n",
            "Test results - Loss: 0.8649963736534119 - Accuracy: 0.5963340401649475%\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.4038 - loss: 1.0842 - val_accuracy: 0.4063 - val_loss: 1.0524\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4990 - loss: 0.9609 - val_accuracy: 0.6018 - val_loss: 0.8789\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7054 - loss: 0.7069 - val_accuracy: 0.5932 - val_loss: 0.9075\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7664 - loss: 0.5809 - val_accuracy: 0.5764 - val_loss: 1.0057\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8014 - loss: 0.5113 - val_accuracy: 0.5769 - val_loss: 1.0720\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8400 - loss: 0.4406 - val_accuracy: 0.5636 - val_loss: 1.1659\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8736 - loss: 0.3668 - val_accuracy: 0.5519 - val_loss: 1.3126\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5960 - loss: 0.8740\n",
            "Test results - Loss: 0.8758004307746887 - Accuracy: 0.590224027633667%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-bcfe919eaa2a>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxnp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mynp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mynp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#we preprocess our train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6896b49247bf>\u001b[0m in \u001b[0;36mprep\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnormalised_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mswr_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalised_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprep_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mswr_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6896b49247bf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnormalised_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mswr_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalised_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprep_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mswr_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4 - Evaluation"
      ],
      "metadata": {
        "id": "DfqtLga5BR47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5 - Paper Overview"
      ],
      "metadata": {
        "id": "9-WoNFmsBUsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6 - Algorithms"
      ],
      "metadata": {
        "id": "vYCgfIFWBXVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "CQgnhlC6LSCc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "random_embedding_dimension = 32\n",
        "glove_embedding_dimension = 200\n",
        "max_length = 40\n",
        "lstm_units_random = 100\n",
        "lstm_units_glove = 128\n",
        "dense_units_random = 32\n",
        "dense_units_glove = 64"
      ],
      "metadata": {
        "id": "uYgK5TC2BduQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(tweets)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(tweets)\n",
        "\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "xnp = np.array(padded_sequences)\n",
        "ynp = np.array(sentiments_numerical)"
      ],
      "metadata": {
        "id": "ZFmBmnqwkm8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_embedding_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size,\n",
        "              output_dim=random_embedding_dimension,\n",
        "              input_length=max_length),\n",
        "    Dropout(0.2),\n",
        "    LSTM(lstm_units_random),\n",
        "    Dense(dense_units_random, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "1ybbToAKLH6D",
        "outputId": "9f6f7799-54c0-43bc-92b8-3814448cfb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe embeddings from stanford\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d /content/glove_data\n",
        "\n",
        "# Set the path to the GloVe file to be used\n",
        "glove_file_path = '/content/glove_data/glove.6B.200d.txt'"
      ],
      "metadata": {
        "id": "yNvzJx79WQr7",
        "outputId": "5bdbc27d-d879-4354-b341-09a4cb33b0b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-28 15:26:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-11-28 15:26:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-11-28 15:26:14--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-11-28 15:28:54 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: /content/glove_data/glove.6B.50d.txt  \n",
            "  inflating: /content/glove_data/glove.6B.100d.txt  \n",
            "  inflating: /content/glove_data/glove.6B.200d.txt  \n",
            "  inflating: /content/glove_data/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings_index = {}\n",
        "with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            glove_embeddings_index[word] = vector\n",
        "\n",
        "glove_matrix = np.zeros((vocab_size, glove_embedding_dimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index < vocab_size:\n",
        "        embedding_vector = glove_embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            glove_matrix[index] = embedding_vector\n",
        "\n",
        "glove_embedding_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size,\n",
        "              output_dim=glove_embedding_dimension,\n",
        "              input_length=max_length,\n",
        "              weights=[glove_matrix],\n",
        "              trainable=True),\n",
        "    Dropout(0.4),\n",
        "    LSTM(lstm_units_glove),\n",
        "    Dense(dense_units_glove, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "nDAGoDXQL1Uh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xnp, ynp, test_size=0.2, random_state=42)\n",
        "\n",
        "random_embedding_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = random_embedding_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = random_embedding_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "-wY2St-KML7f",
        "outputId": "e62ff0be-b7ba-42c0-c0b2-0c4404f248eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.5342 - loss: 0.0000e+00 - val_accuracy: 0.4664 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.4987 - loss: 0.0000e+00 - val_accuracy: 0.4659 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.5236 - loss: 0.0000e+00 - val_accuracy: 0.4608 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.5298 - loss: 0.0000e+00 - val_accuracy: 0.4455 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.5213 - loss: 0.0000e+00 - val_accuracy: 0.4598 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.5152 - loss: 0.0000e+00 - val_accuracy: 0.4679 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.5336 - loss: 0.0000e+00 - val_accuracy: 0.4430 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.4917 - loss: 0.0000e+00 - val_accuracy: 0.4598 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.5255 - loss: 0.0000e+00 - val_accuracy: 0.4511 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.5432 - loss: 0.0000e+00 - val_accuracy: 0.4470 - val_loss: 0.0000e+00\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4691 - loss: 0.0000e+00\n",
            "Test Loss: 0.0\n",
            "Test Accuracy: 0.4704684317111969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(tweets)\n",
        "sequences = tokenizer.texts_to_sequences(tweets)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "xnp = np.array(padded_sequences)\n",
        "ynp = np.array(sentiments_numerical)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xnp, ynp, test_size=0.2, random_state=42)\n",
        "\n",
        "glove_embedding_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = glove_embedding_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = glove_embedding_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "uZn2_dXtTi-k",
        "outputId": "ef181a69-ee13-406d-9a8d-a30a12926244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 187ms/step - accuracy: 0.3109 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 189ms/step - accuracy: 0.2994 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 191ms/step - accuracy: 0.2946 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 184ms/step - accuracy: 0.3078 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 196ms/step - accuracy: 0.3002 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 180ms/step - accuracy: 0.3101 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 185ms/step - accuracy: 0.3009 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 190ms/step - accuracy: 0.3134 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 196ms/step - accuracy: 0.3179 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 192ms/step - accuracy: 0.2931 - loss: 0.0000e+00 - val_accuracy: 0.2790 - val_loss: 0.0000e+00\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.3211 - loss: 0.0000e+00\n",
            "Test Loss: 0.0\n",
            "Test Accuracy: 0.31486761569976807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7 - Evaluation"
      ],
      "metadata": {
        "id": "wMW5xGIiBa7-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Ws00iF1BdF9"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}